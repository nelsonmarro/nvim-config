return {
  -- {
  --   "yetone/avante.nvim",
  --   -- if you want to build from source then do `make BUILD_FROM_SOURCE=true`
  --   -- ⚠️ must add this setting! ! !
  --   build = "make",
  --   event = "VeryLazy",
  --   version = false, -- Never set this value to "*"! Never!
  --   ---@module 'avante'
  --   ---@type avante.Config
  --   opts = {
  --     -- add any opts here
  --     behaviour = {
  --       enable_fastapply = true, -- Enable Fast Apply feature
  --     }, -- for example
  --     web_search_engine = {
  --       provider = "google", -- tavily, serpapi, google, kagi, brave, or searxng
  --       proxy = nil, -- proxy support, e.g., http://127.0.0.1:7890
  --     },
  --     mode = "agentic",
  --     provider = "gemini",
  --     providers = {
  --       gemini = {
  --         endpoint = "https://generativelanguage.googleapis.com/v1beta/models",
  --         model = "gemini-2.5-pro",
  --         timeout = 30000, -- Timeout in milliseconds
  --         extra_request_body = {
  --           temperature = 1,
  --           max_tokens = 65536,
  --         },
  --       },
  --       morph = {
  --         model = "morph-v3-large",
  --       },
  --     },
  --     rag_service = { -- RAG Service configuration
  --       enabled = false, -- Enables the RAG service
  --       host_mount = os.getenv("HOME"), -- Host mount path for the rag service (Docker will mount this path)
  --       runner = "docker", -- Runner for the RAG service (can use docker or nix)
  --       llm = { -- Language Model (LLM) configuration for RAG service
  --         provider = "gemini", -- LLM provider
  --         endpoint = "https://generativelanguage.googleapis.com/v1beta/models", -- LLM API endpoint
  --         api_key = "GEMINI_API_KEY", -- Environment variable name for the LLM API key
  --         model = "gemini-2.5-pro", -- LLM model name
  --         extra = nil, -- Additional configuration options for LLM
  --       },
  --       embed = { -- Embedding model configuration for RAG service
  --         provider = "openai", -- Embedding provider
  --         endpoint = "https://generativelanguage.googleapis.com/v1beta/models", -- Embedding API endpoint
  --         api_key = "GEMINI_API_KEY", -- Environment variable name for the embedding API key
  --         model = "gemini-embedding-001", -- Embedding model name
  --         extra = nil, -- Additional configuration options for the embedding model
  --       },
  --       docker_extra_args = "", -- Extra arguments to pass to the docker command
  --     },
  --   },
  --   dependencies = {
  --     "nvim-lua/plenary.nvim",
  --     "MunifTanjim/nui.nvim",
  --     --- The below dependencies are optional,
  --     "echasnovski/mini.pick", -- for file_selector provider mini.pick
  --     "nvim-telescope/telescope.nvim", -- for file_selector provider telescope
  --     "hrsh7th/nvim-cmp", -- autocompletion for avante commands and mentions
  --     "ibhagwan/fzf-lua", -- for file_selector provider fzf
  --     "stevearc/dressing.nvim", -- for input provider dressing
  --     "folke/snacks.nvim", -- for input provider snacks
  --     "nvim-tree/nvim-web-devicons", -- or echasnovski/mini.icons
  --     "zbirenbaum/copilot.lua", -- for providers='copilot'
  --     {
  --       -- support for image pasting
  --       "HakonHarnes/img-clip.nvim",
  --       event = "VeryLazy",
  --       opts = {
  --         -- recommended settings
  --         default = {
  --           embed_image_as_base64 = false,
  --           prompt_for_file_name = false,
  --           drag_and_drop = {
  --             insert_mode = true,
  --           },
  --           -- required for Windows users
  --           use_absolute_path = true,
  --         },
  --       },
  --     },
  --     {
  --       -- Make sure to set this up properly if you have lazy=true
  --       "MeanderingProgrammer/render-markdown.nvim",
  --       opts = {
  --         file_types = { "markdown", "Avante" },
  --       },
  --       ft = { "markdown", "Avante" },
  --     },
  --   },
  -- },
}
